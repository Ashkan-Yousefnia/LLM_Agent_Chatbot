{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c9953d",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"padding: 28px; border: 2px solid #e7eaf6; border-radius: 18px; background: linear-gradient(135deg,#f8fbff, #fff7fd); box-shadow: 0 10px 30px rgba(0,0,0,0.06);\">\n",
    "\n",
    "\n",
    "\n",
    "  <h2 style=\"margin: 0; font-weight: 600;\">\n",
    "    🤖 LLM Agent\n",
    "  </h2>\n",
    "\n",
    "  <hr style=\"border: none; height: 1px; background: #e7eaf6; width: 82%; margin: 14px 0 10px;\" />\n",
    "\n",
    "  <p style=\"font-size: 1.12em; margin: 0.3em 0 0;\">\n",
    "    <strong>Ashkan Yousefnia</strong>\n",
    "  </p>\n",
    "\n",
    "\n",
    "\n",
    "  <p style=\"font-size: 0.98em; margin: 0.6em 0 0; opacity: 0.75;\">\n",
    "    <em>Exploring intelligent agents powered by large language models.</em>\n",
    "  </p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<!-- Plain Markdown fallback if HTML styles are stripped:\n",
    "# Deep Learning Project\n",
    "## LLM Agent\n",
    "**Ashkan Yousefnia**  \n",
    "Student ID: `401102829`\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55687044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "import fitz\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "import faiss\n",
    "import numpy as np\n",
    "metis_api_key='tpsg-UMjoKAcsNZZ1u9YHcIyFlCLnh2zlT02'\n",
    "import re\n",
    "import os\n",
    "# import msvcrt\n",
    "# import keyboard  \n",
    "from types import SimpleNamespace\n",
    "\n",
    "client = OpenAI(api_key = metis_api_key, base_url=\"https://api.metisai.ir/openai/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec0a5fa",
   "metadata": {},
   "source": [
    "This Agent uses local Tokenizer but the model is used via api call(there is a version using Mistral model at the end of this code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, api_token=\"tpsg-UMjoKAcsNZZ1u9YHcIyFlCLnh2zlT02\"):\n",
    "        self.api_token = api_token\n",
    "        self.model = \"gpt-4.1-mini\"\n",
    "        self.client = OpenAI(api_key=self.api_token, base_url=\"https://api.metisai.ir/openai/v1\")\n",
    "        self.conversation = []  # List of (role, content)\n",
    "        self.summary = None\n",
    "        self.max_turns_before_summarizing = 2  # Number of user-assistant pairs\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def capture_the_flag(self, prompt: str) -> str:\n",
    "        # Add current user prompt to conversation\n",
    "        reply = \"\"\n",
    "        self.conversation.append((\"user\", prompt))\n",
    "        \n",
    "        \n",
    "        # Check if summarization is needed\n",
    "        \n",
    "        if self.needs_summarization():\n",
    "            self.update_memory()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Build prompt with context\n",
    "        if self.check_if_pdf_input(prompt):\n",
    "            \n",
    "             \n",
    "            path=self.extract_local_pdf_paths(prompt)\n",
    "            chunks,embeddings=self.pdf_address_to_chunk_embedding(address=path)\n",
    "            index, chunks=self.build_or_load_faiss_index(chunks=chunks,embeddings=embeddings)\n",
    "\n",
    "\n",
    "            query = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"I am going to give you a prompt. After reading the prompt write a single query based on the given prompt so that I can look for the answer of that query in the pdf file. prompt:-{prompt}-. Just print the query.\"}],\n",
    "                max_tokens=300,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "            query = query.choices[0].message.content.strip()\n",
    "\n",
    "            \n",
    "\n",
    "            results = self.search_faiss_index(index, chunks, query)\n",
    "            \n",
    "\n",
    "            reply = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"This is the query that I have: {query}.These are the relevant answers I found based on the query: {results}. Now based on the provided information give a answer to the query.\"}],\n",
    "                max_tokens=300,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            reply = reply.choices[0].message.content.strip()\n",
    "            return reply\n",
    "\n",
    "        elif self.game_mode(prompt=prompt):\n",
    "            try:\n",
    "                play()\n",
    "            except SystemExit as e:\n",
    "                # Avoid printing traceback in Jupyter when exiting with a message\n",
    "                if isinstance(e.code, str):\n",
    "                    print(e.code)\n",
    "                raise  # re-raise so IPython doesn't crash with weird traceback\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nGame aborted. Bye!\")\n",
    "            \n",
    "\n",
    "        elif self.check_for_web_search(prompt=prompt):\n",
    "\n",
    "            reply=self.web_searcher(prompt)\n",
    "\n",
    "            self.conversation.append((\"assistant\", reply))\n",
    "            return reply\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            messages = self.build_prompt(prompt)\n",
    "\n",
    "            # Call model\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                max_tokens=300,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "            reply = response.choices[0].message.content.strip()\n",
    "            self.conversation.append((\"assistant\", reply))\n",
    "\n",
    "        return reply\n",
    "\n",
    "    def needs_summarization(self) -> bool:\n",
    "        # If too many total messages (user+assistant), trigger summarization\n",
    "        return len(self.conversation) > self.max_turns_before_summarizing * 2\n",
    "\n",
    "    def update_memory(self):\n",
    "        # Summarize everything except the most recent few turns\n",
    "        old_convo = self.conversation[:-self.max_turns_before_summarizing]\n",
    "        if not old_convo:\n",
    "            return\n",
    "\n",
    "        convo_text = \"\\n\".join(f\"{role.capitalize()}: {content}\" for role, content in old_convo)\n",
    "\n",
    "        summary_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a summarizer.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following conversation for memory:\\n\\n{convo_text}\"}\n",
    "        ]\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=summary_prompt,\n",
    "            max_tokens=200,\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "        self.summary = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Retain only the recent portion of the conversation\n",
    "        self.conversation = self.conversation[-self.max_turns_before_summarizing:]\n",
    "\n",
    "    def build_prompt(self, current_user_input: str):\n",
    "\n",
    "        messages = []\n",
    "        \n",
    "        if self.summary:\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"Summary of prior conversation: {self.summary}\"\n",
    "            })\n",
    "\n",
    "        # Add recent conversation history\n",
    "        for role, content in self.conversation:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "        # Add current prompt again as the latest user message\n",
    "        # (optional if already added in self.conversation)\n",
    "        # messages.append({\"role\": \"user\", \"content\": current_user_input})\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def check_for_web_search(self,prompt):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"{prompt}. if you need more data to answer user question only say SEARCH.\"}] ,\n",
    "            max_tokens=100,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        response = response.choices[0].message.content.strip()\n",
    "\n",
    "        #print(response)\n",
    "        if response == 'SEARCH':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def web_searcher(self,prompt):\n",
    "\n",
    "        num_results=3\n",
    "        #message=self.build_prompt(prompt)\n",
    "        query = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"in order to answer the following prompt write the best query so i can search the internet with that query {prompt}. just write the query\"}],\n",
    "            max_tokens=100,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        query=query.choices[0].message.content.strip()\n",
    "        \n",
    "        EXA_API_KEY = \"679619bb-2cb6-4e88-8ee1-33febc5ffbac\"\n",
    "        EXA_ENDPOINT = \"https://api.exa.ai/search\"\n",
    "\n",
    "        headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key\": EXA_API_KEY\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "        payload = {\n",
    "            \"query\": query,\n",
    "            \"num_results\": num_results,\n",
    "            \"include_domains\": [],\n",
    "            \"exclude_domains\": [],\n",
    "            \"use_autoprompt\": True,   # optional for better queries\n",
    "            \"type\": \"auto\"            \n",
    "        }\n",
    "\n",
    "        response = requests.post(EXA_ENDPOINT, json=payload, headers=headers,proxies={})\n",
    "        response.raise_for_status()\n",
    "\n",
    "        results = response.json()[\"results\"]\n",
    "        urls = [item[\"url\"] for item in results]\n",
    "        contents=[]\n",
    "        headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "        }   \n",
    "        \n",
    "        for url in urls:\n",
    "            response = requests.get(url,headers=headers)\n",
    "            response.raise_for_status()  # raises HTTPError if status is 4xx or 5xx\n",
    "            content = response.text\n",
    "            contents.append(content)\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"{query}.for the query that i just gave you i have searched the internet and i am going to provide you with the content of the three most relevant results.find the best answer for the query in the contents of these web sites  {contents}. just write the answer\"}],\n",
    "            max_tokens=100,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        response=response.choices[0].message.content.strip()\n",
    "\n",
    "        return response\n",
    "    \n",
    "    \n",
    "    def pdf_address_to_chunk_embedding(self,address,chunk_len=250):\n",
    "        \n",
    "        \n",
    "       \n",
    "        nltk.download('punkt_tab')\n",
    "  \n",
    "\n",
    "        doc = fitz.open(address)\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "        doc.close()\n",
    "\n",
    "        # Step 2: Tokenize and chunk into 100-word chunks\n",
    "        words = word_tokenize(full_text)\n",
    "        chunks = [' '.join(words[i:i + chunk_len])\n",
    "                  for i in range(0, len(words), chunk_len)]\n",
    "        # compute embeddings as numpy float32 array\n",
    "        embeddings = self.embedding_model.encode(chunks)\n",
    "        embeddings = np.asarray(embeddings, dtype='float32')\n",
    "\n",
    "        return chunks,embeddings\n",
    "    \n",
    "\n",
    "\n",
    "    def build_or_load_faiss_index(self,chunks, embeddings, index_path=\"faiss.index\"):\n",
    "       \n",
    "        dim = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "        index.add(embeddings)                 # now a proper numpy array\n",
    "        faiss.write_index(index, index_path)\n",
    "        return index, chunks\n",
    "\n",
    "    def search_faiss_index(self,index, chunks, query, top_k=3):\n",
    "\n",
    "        q_emb = self.embedding_model.encode([query])\n",
    "        q_emb = np.asarray(q_emb, dtype='float32')\n",
    "        distances, indices = index.search(q_emb, top_k)\n",
    "        print(indices)\n",
    "        return [chunks[i] for i in indices[0]]\n",
    "\n",
    "    def check_if_pdf_input(self,query):\n",
    "        return '.pdf' in query.lower()\n",
    "    \n",
    "    def extract_local_pdf_paths(self,text):\n",
    "        # Match file paths with spaces, until .pdf\n",
    "        pattern = r'([A-Za-z]:[\\\\/](?:[^:*?\"<>|\\r\\n]+[\\\\/])*[^:*?\"<>|\\r\\n]+\\.pdf)'\n",
    "        matches = re.findall(pattern, text)\n",
    "        pdf_paths = [match.strip() for match in matches if os.path.exists(match.strip())]\n",
    "        return pdf_paths[0] if pdf_paths else None\n",
    "    def game_mode(self,prompt):\n",
    "\n",
    "        # response = self.client.chat.completions.create(\n",
    "        #     model=self.model,\n",
    "        #     messages=[{\"role\": \"user\", \"content\": f\"Read this user input: -{prompt}-.If user input contain words like: game/20 question/20 question game/guess, just print : GAME\"}],\n",
    "        #     max_tokens=100,\n",
    "        #     temperature=0.1\n",
    "        # )\n",
    "        \n",
    "        # response=response.choices[0].message.content.strip()\n",
    "\n",
    "        # if response==\"GAME\":\n",
    "        #     return True\n",
    "        \n",
    "        # return False\n",
    "        return '20 question game' in prompt.lower()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54e5ab",
   "metadata": {},
   "source": [
    "The code that is related to the 20 question game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f395fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "\n",
    "from keywords import KEYWORDS_JSON\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from random import choice\n",
    "from string import Template\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "#llm_parent_dir = \"/kaggle/input/flan-t5/pytorch/large\"\n",
    "\n",
    "device = None\n",
    "model = None\n",
    "tokenizer = None\n",
    "model_initialized = False\n",
    "\n",
    "ERROR = \"ERROR\"\n",
    "DONE = \"DONE\"\n",
    "INACTIVE = \"INACTIVE\"\n",
    "ACTIVE = \"ACTIVE\"\n",
    "TIMEOUT = \"TIMEOUT\"\n",
    "\n",
    "GUESS = \"guess\"\n",
    "ASK = \"ask\"\n",
    "GUESSER = \"guesser\"\n",
    "ANSWERER = \"answerer\"\n",
    "\n",
    "RANDOM_GUESSER = \"random_guesser\"\n",
    "RANDOM_ANSWERER = \"random_answerer\"\n",
    "\n",
    "def weighted_random_category(keywords_list):\n",
    "    try:\n",
    "        return random.choices(population=keywords_list, weights=[len(entry[\"words\"]) for entry in keywords_list], k=1)[0]\n",
    "    except:\n",
    "        pass\n",
    "    return random.choice(keywords_list)\n",
    "\n",
    "keywords_list = json.loads(KEYWORDS_JSON)\n",
    "keyword_cat = weighted_random_category(keywords_list)\n",
    "category = keyword_cat[\"category\"]\n",
    "keyword_obj = random.choice(keyword_cat[\"words\"])\n",
    "keyword = keyword_obj[\"keyword\"]\n",
    "alts = keyword_obj[\"alts\"]\n",
    "\n",
    "try:\n",
    "    with open(\"/kaggle_simulations/data.json\") as f:\n",
    "        json_content = f.read()\n",
    "        d_keywords_list = json.loads(json_content)\n",
    "        d_keyword_cat = weighted_random_category(d_keywords_list)\n",
    "        d_category = d_keyword_cat[\"category\"]\n",
    "        d_keyword_obj = random.choice(d_keyword_cat[\"words\"])\n",
    "        d_keyword = d_keyword_obj[\"keyword\"]\n",
    "        d_alts = d_keyword_obj[\"alts\"]\n",
    "        # re-assign\n",
    "        category = d_category\n",
    "        keyword = d_keyword\n",
    "        alts = d_alts\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "def random_guesser(obs):\n",
    "    if obs.turnType == GUESS:\n",
    "        return \"banana\"\n",
    "    if random.random() < .5:\n",
    "        return \"Is is a person?\"\n",
    "    if random.random() < .5:\n",
    "        return \"Is it a place?\"\n",
    "    return \"Is it a thing?\"\n",
    "\n",
    "def random_answerer():\n",
    "    if random.random() > .5:\n",
    "        return \"yes\"\n",
    "    return \"no\"\n",
    "\n",
    "\n",
    "def guesser_agent(obs):\n",
    "    info_prompt = \"\"\"You are playing a game of 20 questions where you ask the questions and try to figure out the keyword, which will be a real or fictional person, place, or thing. \\nHere is what you know so far:\\n{q_a_thread}\"\"\"\n",
    "    questions_prompt = \"\"\"Ask one yes or no question.\"\"\"\n",
    "    guess_prompt = \"\"\"Guess the keyword. Only respond with the exact word/phrase. For example, if you think the keyword is [paris], don't respond with [I think the keyword is paris] or [Is the keyword Paris?]. Respond only with the word [paris].\"\"\"\n",
    "\n",
    "    q_a_thread = \"\"\n",
    "    for i in range(0, len(obs.answers)):\n",
    "        q_a_thread = \"{}Q: {} A: {}\\n\".format(\n",
    "            q_a_thread,\n",
    "            obs.questions[i],\n",
    "            obs.answers[i]\n",
    "        )\n",
    "\n",
    "    prompt = \"\"\n",
    "    if obs.turnType == ASK:\n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(q_a_thread=q_a_thread),\n",
    "            questions_prompt\n",
    "        )\n",
    "    elif obs.turnType == GUESS:\n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(q_a_thread=q_a_thread),\n",
    "            guess_prompt\n",
    "        )\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "    return call_llm(prompt)\n",
    "\n",
    "    \n",
    "\n",
    "def answerer_agent(obs):\n",
    "    info_prompt = \"\"\"You are a very precise answerer in a game of 20 questions. The keyword that the questioner is trying to guess is [the {category} {keyword}]. \"\"\"\n",
    "    answer_question_prompt = \"\"\"Answer the following question with only yes, no, or if unsure maybe: {question}\"\"\"\n",
    "\n",
    "    if obs.turnType == \"answer\":\n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(category=category,keyword=keyword),\n",
    "            answer_question_prompt.format(question=obs.questions[-1])\n",
    "        )\n",
    "        return call_llm(prompt)\n",
    "    else: \n",
    "        return \"\"\n",
    "\n",
    "\n",
    "agents = {GUESSER: guesser_agent, ANSWERER: answerer_agent, RANDOM_ANSWERER: random_answerer, RANDOM_GUESSER: random_guesser}\n",
    "\n",
    "def guesser_action(active, inactive, step):\n",
    "    inactive.observation.keyword = keyword\n",
    "    inactive.observation.category = category\n",
    "    guessed = False\n",
    "    bad_guess = False\n",
    "    if not active.action:\n",
    "        active.status = ERROR\n",
    "        bad_guess = True\n",
    "    elif active.observation.turnType == ASK:\n",
    "        question = active.action[:750]\n",
    "        active.observation.questions.append(question)\n",
    "        inactive.observation.questions.append(question)\n",
    "    elif active.observation.turnType == GUESS:\n",
    "        guess = active.action[:100]\n",
    "        active.observation.guesses.append(guess)\n",
    "        inactive.observation.guesses.append(guess)\n",
    "    if active.action and keyword_guessed(active.action):\n",
    "        guessed = True\n",
    "        score = 20 - int(step / 3)\n",
    "        end_game(active, score, DONE)\n",
    "        end_game(inactive, score, DONE)\n",
    "    return [guessed, bad_guess]\n",
    "\n",
    "def end_game(agent, reward, status):\n",
    "    agent.observation.keyword = keyword\n",
    "    agent.observation.category = category\n",
    "    agent.reward = reward\n",
    "    agent.status = status\n",
    "\n",
    "def answerer_action(active, inactive):\n",
    "    active.observation.keyword = keyword\n",
    "    active.observation.category = category\n",
    "    response = active.action\n",
    "    bad_response = False\n",
    "    if not response:\n",
    "        response = \"none\"\n",
    "        end_game(active, -1, ERROR)\n",
    "        end_game(inactive, 1, DONE)\n",
    "        bad_response = True\n",
    "    elif \"yes\" in response.lower():\n",
    "        response = \"yes\"\n",
    "    elif \"no\" in response.lower():\n",
    "        response = \"no\"\n",
    "    else:\n",
    "        response = \"maybe\"\n",
    "        end_game(active, -1, ERROR)\n",
    "        end_game(inactive, 1, DONE)\n",
    "        bad_response = True\n",
    "    active.observation.answers.append(response)\n",
    "    inactive.observation.answers.append(response)\n",
    "    return bad_response\n",
    "\n",
    "def increment_turn(active, inactive, step, guessed):\n",
    "    if step == 59 and not guessed:\n",
    "        end_game(active, -1, DONE)\n",
    "        end_game(inactive, -1, DONE)\n",
    "    elif active.observation.turnType == \"guess\":\n",
    "        active.observation.turnType = \"ask\"\n",
    "    elif active.observation.turnType == \"ask\":\n",
    "        active.observation.turnType = \"guess\"\n",
    "        active.status = INACTIVE\n",
    "        inactive.status = ACTIVE\n",
    "    else:\n",
    "        active.status = INACTIVE\n",
    "        inactive.status = ACTIVE\n",
    "\n",
    "\n",
    "def interpreter(state, env):\n",
    "    if env.done:\n",
    "        return state\n",
    "\n",
    "    # Isolate the active and inactive agents.\n",
    "    active1 = state[0] if state[0].status == ACTIVE else state[1]\n",
    "    inactive1 = state[0] if state[0].status == INACTIVE else state[1]\n",
    "    active2 = state[2] if state[2].status == ACTIVE else state[3]\n",
    "    inactive2 = state[2] if state[2].status == INACTIVE else state[3]\n",
    "    if active1.status == DONE and inactive1.status == DONE:\n",
    "        active1 = None\n",
    "        inactive1 = None\n",
    "    if active2.status == DONE or inactive2.status == DONE:\n",
    "        active2 = None\n",
    "        inactive2 = None\n",
    "    if active1 is None and inactive1 is None and active2 is None and inactive2 is None:\n",
    "        return state\n",
    "\n",
    "    step = state[0].observation.step\n",
    "\n",
    "    end_early = (active1 and active1.status) in (TIMEOUT, ERROR) or (active2 and active2.status in (TIMEOUT, ERROR))\n",
    "    one_guessed = False\n",
    "    two_guessed = False\n",
    "    one_bad_guess = False\n",
    "    two_bad_guess = False\n",
    "    one_bad_response = False\n",
    "    two_bad_response = False\n",
    "\n",
    "    if active1 is None or active2 is None:\n",
    "        raise ValueError\n",
    "\n",
    "    if active1.observation.role == GUESSER:\n",
    "        [one_guessed, one_bad_guess] = guesser_action(active1, inactive1, step)\n",
    "    else:\n",
    "        one_bad_response = answerer_action(active1, inactive1)\n",
    "\n",
    "    if active2.observation.role == GUESSER:\n",
    "        [two_guessed, two_bad_guess] = guesser_action(active2, inactive2, step)\n",
    "    else:\n",
    "        two_bad_response = answerer_action(active2, inactive2)\n",
    "\n",
    "    if active1.status in (TIMEOUT, ERROR) or one_bad_response or one_bad_guess:\n",
    "        end_game(active1, -1, active1.status)\n",
    "        end_game(inactive1, 1, DONE)\n",
    "    elif end_early or two_bad_response or two_bad_guess:\n",
    "        end_game(active1, 1, DONE)\n",
    "        end_game(inactive1, 1, DONE)\n",
    "    else:\n",
    "        increment_turn(active1, inactive1, step, one_guessed)\n",
    "    \n",
    "    if active2.status in (TIMEOUT, ERROR) or two_bad_response or two_bad_guess:\n",
    "        end_game(active2, -1, active2.status)\n",
    "        end_game(inactive2, 1, DONE)\n",
    "    elif end_early or one_bad_response or one_bad_guess:\n",
    "        end_game(active2, 1, DONE)\n",
    "        end_game(inactive2, 1, DONE)\n",
    "    else:\n",
    "        increment_turn(active2, inactive2, step, two_guessed)\n",
    "    \n",
    "    # make sure to end the game if only one team guessed correctly this round\n",
    "    if one_guessed and not two_guessed:\n",
    "        end_game(active2, 0, DONE)\n",
    "        end_game(inactive2, 0, DONE)\n",
    "    elif two_guessed and not one_guessed:\n",
    "        end_game(active1, 0, DONE)\n",
    "        end_game(inactive1, 0, DONE)\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def renderer(state, env):\n",
    "\n",
    "    for s in state:\n",
    "        print(\"role: \", s.observation.role)\n",
    "        if s.observation.role == GUESSER:\n",
    "            transcript = \"\"\n",
    "            for i in range(0, len(s.observation.guesses)):\n",
    "                transcript = \"{}Q: {} A: {}\\nG: {}\\n\".format(\n",
    "                    transcript, s.observation.questions[i],\n",
    "                    s.observation.answers[i],\n",
    "                    s.observation.guesses[i]\n",
    "                )\n",
    "            print(transcript)\n",
    "\n",
    "        print(\"keyword: \", s.observation.keyword)\n",
    "        print(\"score: \", s.reward)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "jsonpath = os.path.abspath(os.path.join(os.getcwd(), \"llm_20_questions.json\"))\n",
    "with open(jsonpath) as f:\n",
    "    specification = json.load(f)\n",
    "\n",
    "def html_renderer():\n",
    "    jspath = path.abspath(path.join(path.dirname(__file__), \"llm_20_questions.js\"))\n",
    "    with open(jspath, encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    t = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return s.lower().replace(\"the\", \"\").replace(\" \", \"\").translate(t)\n",
    "\n",
    "def compare_words(a, b) -> bool:\n",
    "    a = normalize(a)\n",
    "    b = normalize(b)\n",
    "    if a == b:\n",
    "        return True\n",
    "    # don't check for plurals if string is too short\n",
    "    if len(a) < 3 or len(b) < 3:\n",
    "        return False\n",
    "    # accept common plurals\n",
    "    if a[-1] == \"s\" and a[:-1] == b:\n",
    "        return True\n",
    "    if b[-1] == \"s\" and a == b[:-1]:\n",
    "        return True\n",
    "    if a[-2:] == \"es\" and a[:-2] == b:\n",
    "        return True\n",
    "    if b[-2:] == \"es\" and a == b[:-2]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def keyword_guessed(guess: str) -> bool:\n",
    "    if compare_words(guess, keyword):\n",
    "      return True\n",
    "    for s in alts:\n",
    "      if compare_words(s, guess):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    global model_initialized\n",
    "    global device\n",
    "    global model\n",
    "    global tokenizer\n",
    "\n",
    "   \n",
    "        \n",
    "    \n",
    "\n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    # outputs = model.generate(**inputs)\n",
    "    # answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    api_token=\"tpsg-7SfPTJFEkQZNDYc7NjLOZ3c34UDuCqM\"\n",
    "    api_token = api_token\n",
    "    model = \"gpt-4.1-mini\"\n",
    "    client = OpenAI(api_key=api_token, base_url=\"https://api.metisai.ir/openai/v1\")\n",
    "    answer = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=300,\n",
    "                temperature=0.7\n",
    "            )\n",
    "    answer = answer.choices[0].message.content.strip()\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c28c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "play_20_questions.py\n",
    "\n",
    "Driver for 20Q with *two* distinct guess-lists:\n",
    "  - question_guesses: the agent’s prior keyword guesses\n",
    "  - answer_guesses:   your prior yes/no/maybe replies\n",
    "\n",
    "Both lists are passed into the LLM every time so it can use all history.\n",
    "This enhanced version maintains a normalized set of past guesses to prevent duplicates,\n",
    "and preserves the original guess text (including casing) when displayed.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# from llm_20_questions import guesser_agent, ASK, GUESS\n",
    "\n",
    "# Constants for turn types\n",
    "ASK = \"ask\"\n",
    "GUESS = \"guess\"\n",
    "\n",
    "\n",
    "def normalize_guess(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a guess string for comparison (case-folded, stripped).\n",
    "    \"\"\"\n",
    "    return s.casefold().strip()\n",
    "\n",
    "\n",
    "def play():\n",
    "    questions = []\n",
    "    answer_guesses = []\n",
    "    question_guesses = []\n",
    "    past_guesses_norm = set()  # normalized past guesses\n",
    "    qnum = 1\n",
    "\n",
    "    print(\"\\nWelcome! Think of a person/place/thing; I'll try to guess it in 20 questions.\\n\")\n",
    "    print(\"Type 'exit' at any time to quit the game.\\n\")\n",
    "\n",
    "    while qnum <= 20:\n",
    "        # ASK turn --------------------------------------------------------\n",
    "        obs = SimpleNamespace(\n",
    "            turnType=ASK,\n",
    "            questions=questions,\n",
    "            answers=answer_guesses,\n",
    "            question_guesses=question_guesses,\n",
    "            answer_guesses_list=answer_guesses,\n",
    "        )\n",
    "        question = guesser_agent(obs).strip()\n",
    "        print(f\"Q{qnum}. {question}\")\n",
    "        sys.stdout.flush()\n",
    "        questions.append(question)\n",
    "\n",
    "        # user's yes/no/maybe reply\n",
    "        ans = \"\"\n",
    "        while ans.lower() not in (\"yes\", \"no\", \"maybe\"):\n",
    "            ans = input(\"Your answer (yes/no/maybe): \").strip()\n",
    "            if ans.lower() == \"exit\":\n",
    "                sys.exit(\"\\nGame aborted by user. Bye!\")\n",
    "        answer_guesses.append(ans.lower())\n",
    "\n",
    "        # GUESS turn ------------------------------------------------------\n",
    "        obs = SimpleNamespace(\n",
    "            turnType=GUESS,\n",
    "            questions=questions,\n",
    "            answers=answer_guesses,\n",
    "            question_guesses=question_guesses,\n",
    "            answer_guesses_list=answer_guesses,\n",
    "        )\n",
    "        # Request and validate a new guess\n",
    "        tries = 0\n",
    "        guess = guesser_agent(obs).strip()\n",
    "        norm = normalize_guess(guess)\n",
    "        # Avoid duplicates by normalized comparison\n",
    "        \n",
    "\n",
    "        # Record this guess\n",
    "        question_guesses.append(guess)\n",
    "        past_guesses_norm.add(norm)\n",
    "        guess_idx = len(question_guesses)\n",
    "        print(f\"\\nGuess #{guess_idx}: {guess}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # was it right?\n",
    "        ok = \"\"\n",
    "        while ok.lower() not in (\"yes\", \"no\"):\n",
    "            try:\n",
    "                ok = input(\"Did I get it right? (yes/no): \").strip()\n",
    "                if ok.lower() == \"exit\":\n",
    "                    sys.exit(\"\\nGame aborted by user. Bye!\")\n",
    "            except (EOFError, KeyboardInterrupt):\n",
    "                sys.exit(\"\\nGame aborted. Bye!\")\n",
    "        if ok.lower() == \"yes\":\n",
    "            print(f\"\\n🎉 Hooray! I got it in {guess_idx} guesses.\")\n",
    "            return\n",
    "\n",
    "        # next round\n",
    "        qnum += 1\n",
    "        print(\"\\n\" + \"— \" * 10 + \"\\n\")\n",
    "\n",
    "    print(\"\\n😔 I couldn't guess it within 20 questions. Well played!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6bbd1",
   "metadata": {},
   "source": [
    "# Testing the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cef4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674e55e",
   "metadata": {},
   "source": [
    "# 1.Agent's History and summarization of history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f78e2bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a language model AI, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('hi how are you?')\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0613c5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('user', 'hi how are you?'), ('assistant', \"Hello! I'm doing great, thank you. How can I assist you today?\")]\n"
     ]
    }
   ],
   "source": [
    "print(agent.conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b09dd5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Ashkan! Nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('hi my name is Ashkan')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6816e38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My job is to assist you by answering questions, providing information, helping with writing, brainstorming ideas, explaining concepts, and much more. Basically, I'm here to help you with a wide range of tasks using natural language. How can I help you today, Ashkan?\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('what is your job?')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c72bd274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('assistant', 'Hi Ashkan! Nice to meet you. How can I help you today?'), ('user', 'what is your job?'), ('assistant', \"My job is to assist you by providing information, answering questions, helping with creative writing, offering explanations, and supporting you in a wide range of topics. Basically, I'm here to help make your tasks easier and provide useful, accurate, and friendly assistance. How can I assist you today?\")]\n"
     ]
    }
   ],
   "source": [
    "print(agent.conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87b258a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don’t know your name unless you tell me. \n",
      "\n",
      "As for my abilities, I can help with a wide range of tasks including:\n",
      "\n",
      "- Answering questions and providing explanations on many topics\n",
      "- Assisting with writing, editing, and brainstorming\n",
      "- Offering tutoring and educational support\n",
      "- Creating summaries and outlines\n",
      "- Generating ideas for projects, stories, or other creative work\n",
      "- Helping with coding and debugging\n",
      "- Translating languages\n",
      "- Providing recommendations and advice\n",
      "\n",
      "Let me know how I can assist you!\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('what are your abilities?')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afd786b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('assistant', 'Hi Ashkan! Nice to meet you. How can I help you today?'), ('user', 'what is your job?'), ('assistant', 'My job is to assist you by answering questions, providing information, helping with tasks, generating ideas, and engaging in conversations on a wide range of topics. How can I assist you today, Ashkan?')]\n"
     ]
    }
   ],
   "source": [
    "print(agent.conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c5a9d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user greeted the assistant and introduced themselves as Ashkan.\n"
     ]
    }
   ],
   "source": [
    "print(agent.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74f78c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Ashkan.\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('what is my name?')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ab8a9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('user', 'Where is Iran located'), ('assistant', 'Iran is located in Western Asia, on the eastern side of the Middle East. It is bordered by several countries: to the northwest by Armenia and Azerbaijan, to the north by the Caspian Sea, to the northeast by Turkmenistan, to the east by Afghanistan and Pakistan, to the south by the Persian Gulf and the Gulf of Oman, and to the west by Turkey and Iraq. Its strategic location connects the Middle East with Central Asia and South Asia.'), ('user', 'Who was Adolf Hitler?'), ('assistant', \"Adolf Hitler was a German political leader who became the dictator of Germany from 1934 to 1945. He was the leader of the National Socialist German Workers' Party (Nazi Party) and played a central role in the events leading up to and during World War II. Hitler promoted a totalitarian regime based on fascist and nationalist ideologies, including aggressive expansionism and racist policies, most notoriously anti-Semitism, which led to the Holocaust—the systematic genocide of six million Jews and millions of other victims.\\n\\nHitler rose to power during a period of economic hardship and political instability in Germany after World War I. He became Chancellor in 1933 and quickly consolidated power, establishing a dictatorship. His policies led to the invasion of multiple countries and the outbreak of World War II in 1939. The war ended in 1945 with Germany's defeat, and Hitler died by suicide in April 1945 as Allied forces closed in on Berlin.\")]\n"
     ]
    }
   ],
   "source": [
    "print(agent.conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec738d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user greeted the assistant and asked about its job and abilities. The assistant explained that its role is to assist by providing information, answering questions, and helping with various tasks. It listed abilities such as answering factual questions, aiding with writing and editing, explaining concepts, assisting with problem-solving, generating creative content, and offering recommendations. The assistant invited the user to specify any particular help needed.\n"
     ]
    }
   ],
   "source": [
    "print(agent.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc0021",
   "metadata": {},
   "source": [
    "Now here is the conversation summarization which happens after each 5 prompt and is given to the model with new instruction to help it remember the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ed35621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(agent.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70024c3f",
   "metadata": {},
   "source": [
    "# 2.Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbe9e6",
   "metadata": {},
   "source": [
    "Now in order to test my Agent's ability to process pdf files I gave it the Large Language Diffusion Models paper which exists in the the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d609420",
   "metadata": {},
   "source": [
    "IMPORTANT: Inorder to not get an error please change the address of the given paper accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0738f941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45 31 51]]\n",
      "The title of the paper in the file \"Large Language Diffusion Models.pdf\" is \"Large Language Diffusion Models\".\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('what is the title of this paper d://uni stuff//Semester 6//Deep Learning//Project//Large Language Diffusion Models.pdf')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d14ddf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45 41 64]]\n",
      "The paper titled \"Large Language Diffusion Models\" explores the application of diffusion models within the realm of large language models (LLMs). Unlike traditional autoregressive or masked language models, diffusion-based approaches model the generation process as a gradual denoising or refinement of text sequences, offering a novel perspective for language modeling.\n",
      "\n",
      "Key insights from the paper and related works include:\n",
      "\n",
      "1. **Diffusion Models for Text**: The paper discusses adapting diffusion processes—originally successful in continuous domains like images—to discrete textual data, addressing challenges such as discrete token representation and score matching for discrete distributions.\n",
      "\n",
      "2. **Modeling Conditional Distributions**: Techniques such as discrete diffusion with self-conditioning (e.g., \"Text-to-text self-conditioned simplex diffusion\") enable the model to better capture conditional dependencies in language generation tasks.\n",
      "\n",
      "3. **Performance and Versatility**: Large language diffusion models demonstrate competitive capabilities in various benchmarks, including reasoning tasks and code generation, by leveraging the flexibility of diffusion frameworks. They can also be applied beyond natural language, such as protein sequence learning, indicating their versatility.\n",
      "\n",
      "4. **Inference Strategies**: The paper details inference methods like low-confidence remasking and semi-autoregressive remasking strategies to optimize generation quality and efficiency during sampling from diffusion-based LLMs.\n",
      "\n",
      "5. **Scaling and Training**: Recent advances highlight scaling masked diffusion models on text, improving their effectiveness at larger scales, and integrating human feedback for instruction following.\n",
      "\n",
      "In summary, \"Large Language Diffusion Models\" present a\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('can you give a summary of this paper d://uni stuff//Semester 6//Deep Learning//Project//Large Language Diffusion Models.pdf')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c94d2058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45 31 51]]\n",
      "The authors of the paper titled \"Large Language Diffusion Models\" are:\n",
      "\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Pierre Albert, Abdelrahman Almahairi, Younes Babaei, Nikita Bashlykov, Shagun Batra, Pranav Bhargava, Shubham Bhosale, et al.\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('Who wrote the paper d://uni stuff//Semester 6//Deep Learning//Project//Large Language Diffusion Models.pdf')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5df3c",
   "metadata": {},
   "source": [
    "# 3.Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7f65a",
   "metadata": {},
   "source": [
    "# 3.1 Web Search Trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5785d",
   "metadata": {},
   "source": [
    "IMPORTANT: If your query reaches an error it means one of the web pages that the model wants to access is either filtered or doesn't allow Iranian IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06987e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.check_for_web_search('What time is it in Iran right now?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bfbf8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current local time in Iran (Tehran) is 20:21:50 IRST (Iran Standard Time), which is UTC +3:30. Iran does not observe daylight saving time as of 2025.\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('What time is it in Iran right now?')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9eb93299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.check_for_web_search('who is the current president of Iran right now?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc029291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of June 2024, the current president of Iran is Ebrahim Raisi. He has been in office since August 2021.\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('who is the current president of Iran right now?')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e860868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.check_for_web_search('Who won the last chess Esport tournament and where was it held')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77ddfbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnus Carlsen won the latest chess esports tournament, the first ever edition of the Chess Esports World Cup. The tournament was held in Riyadh.\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('Who won the last chess Esport tournament and where was it held')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70bd6c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester United's last game result was a 0-1 loss against Arsenal on Sunday, August 17, 2025.\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('What was the result of the last Manchester United game and which team was it against')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cca840",
   "metadata": {},
   "source": [
    "# 3.2 The 20 Question Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5489f6",
   "metadata": {},
   "source": [
    "IMPORTANT : inorder to go into the game your prompt must cantain 20 question game and you should type your answer as yes/no/maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27864486",
   "metadata": {},
   "source": [
    "type exit to quit the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220966e1",
   "metadata": {},
   "source": [
    "First Word : Friedrich Nietzsche "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c376286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome! Think of a person/place/thing; I'll try to guess it in 20 questions.\n",
      "\n",
      "Type 'exit' at any time to quit the game.\n",
      "\n",
      "Q1. Is the keyword a living person?\n",
      "\n",
      "Guess #1: Elon Musk\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q2. Is the keyword a fictional character?\n",
      "\n",
      "Guess #2: Barack Obama\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q3. Is the keyword a currently living person?\n",
      "\n",
      "Guess #3: Albert Einstein\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q4. Was the person primarily known for their work in the 20th century?\n",
      "\n",
      "Guess #4: napoleon bonaparte\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q5. Was the person primarily known for their work before the 20th century?\n",
      "\n",
      "Guess #5: leonardo da vinci\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q6. Was this person primarily known for their work in the 19th century?\n",
      "\n",
      "Guess #6: abraham lincoln\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q7. Was this person primarily known for their contributions in the field of science or technology?\n",
      "\n",
      "Guess #7: charles dickens\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q8. Was this person primarily known for their contributions in the field of politics or government?\n",
      "\n",
      "Guess #8: charles dickens\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q9. Was this person primarily known for their contributions in the field of literature or the arts?\n",
      "\n",
      "Guess #9: napoleon bonaparte\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q10. Was this person primarily known for their contributions in the field of exploration or geography?\n",
      "\n",
      "Guess #10: abraham lincoln\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q11. Was this person primarily known for their contributions in the field of religion or philosophy?\n",
      "\n",
      "Guess #11: friedrich nietzsche\n",
      "\n",
      "🎉 Hooray! I got it in 11 guesses.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.capture_the_flag('lets play the 20 question game')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa0b0f",
   "metadata": {},
   "source": [
    "Second word : Lamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c256cffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome! Think of a person/place/thing; I'll try to guess it in 20 questions.\n",
      "\n",
      "Type 'exit' at any time to quit the game.\n",
      "\n",
      "Q1. Is the keyword a person?\n",
      "\n",
      "Guess #1: Eiffel Tower\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q2. Is the keyword a place?\n",
      "\n",
      "Guess #2: car\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q3. Is the keyword a thing?\n",
      "\n",
      "Guess #3: computer\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q4. Is the thing man-made?\n",
      "\n",
      "Guess #4: smartphone\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q5. Is the thing commonly found indoors?\n",
      "\n",
      "Guess #5: computer\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q6. Is the thing electronic?\n",
      "\n",
      "Guess #6: smartphone\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q7. Is the electronic thing primarily used for communication?\n",
      "\n",
      "Guess #7: television\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q8. Is the electronic thing primarily used for entertainment?\n",
      "\n",
      "Guess #8: microwave\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q9. Is the electronic thing primarily used for work or productivity?\n",
      "\n",
      "Guess #9: microwave\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q10. Is the electronic thing primarily used for household chores or maintenance?\n",
      "\n",
      "Guess #10: calculator\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q11. Is the electronic thing primarily used for health or personal care?\n",
      "\n",
      "Guess #11: calculator\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q12. Is the electronic thing primarily used for cooking or food preparation?\n",
      "\n",
      "Guess #12: calculator\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q13. Is the electronic thing primarily used for lighting or illumination?\n",
      "\n",
      "Guess #13: lamp\n",
      "\n",
      "🎉 Hooray! I got it in 13 guesses.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.capture_the_flag('wanna play the 20 question game?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dfa547e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome! Think of a person/place/thing; I'll try to guess it in 20 questions.\n",
      "\n",
      "Type 'exit' at any time to quit the game.\n",
      "\n",
      "Q1. Is the keyword a person?\n",
      "\n",
      "Guess #1: eiffel tower\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q2. Is the keyword a place?\n",
      "\n",
      "Guess #2: book\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q3. Is the keyword a fictional thing?\n",
      "\n",
      "Guess #3: thing\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q4. Is the keyword a real thing?\n",
      "\n",
      "Guess #4: computer\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q5. Is the keyword a living thing?\n",
      "\n",
      "Guess #5: dog\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q6. Is the keyword an animal?\n",
      "\n",
      "Guess #6: dog\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q7. Is the animal a mammal?\n",
      "\n",
      "Guess #7: eagle\n",
      "\n",
      "— — — — — — — — — — \n",
      "\n",
      "Q8. Is the animal a bird?\n",
      "\n",
      "Guess #8: shark\n",
      "\n",
      "🎉 Hooray! I got it in 8 guesses.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.capture_the_flag('lets play the 20 question game')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425660e",
   "metadata": {},
   "source": [
    "IMPORTANT : If the evaluate_20Q.py didn't work properly you can easily give the keywords to the model here and get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b323a97",
   "metadata": {},
   "source": [
    "# 4.GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41eaadf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "def run_ctf(prompt: str):\n",
    "    try:\n",
    "        return agent.capture_the_flag(prompt)\n",
    "    except SystemExit as e:\n",
    "        # In case game_mode/play() calls exit\n",
    "        return str(e) if isinstance(e.code, str) else \"Game ended.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {type(e).__name__}: {e}\"\n",
    "\n",
    "# ----- Styling (beauty only) -----\n",
    "CSS = \"\"\"\n",
    ":root { --radius-xl: 18px; }\n",
    "#header {\n",
    "  background: linear-gradient(135deg,#111827 0%,#0ea5e9 50%,#10b981 100%);\n",
    "  padding: 22px 18px; border-radius: var(--radius-xl); color: white;\n",
    "  box-shadow: 0 10px 28px rgba(0,0,0,.25);\n",
    "}\n",
    "#header h2 { margin: 0 0 4px 0; font-size: 1.4rem; }\n",
    "#header p { margin: 0; opacity: .95; }\n",
    "#prompt_box textarea { min-height: 90px; font-size: 16px; }\n",
    "#result_box textarea { min-height: 360px; font-size: 15px; }\n",
    "footer { visibility: hidden; } /* cleaner look */\n",
    "\"\"\"\n",
    "\n",
    "theme = gr.themes.Soft(\n",
    "    primary_hue=\"teal\",\n",
    "    secondary_hue=\"slate\",\n",
    ").set(\n",
    "    block_shadow=\"*shadow_drop_lg\",\n",
    "    block_border_width=\"1px\",\n",
    "    button_primary_background_fill=\"*primary_600\",\n",
    "    button_primary_background_fill_hover=\"*primary_700\",\n",
    "    body_text_size=\"16px\",\n",
    ")\n",
    "\n",
    "with gr.Blocks(title=\"CTF Agent (Minimal)\", theme=theme, css=CSS) as demo:\n",
    "    with gr.Group(elem_id=\"header\"):\n",
    "        gr.HTML(\"\"\"\n",
    "            <h2>🏴‍☠️ CTF Agent</h2>\n",
    "            <p>Every input goes straight to <code>capture_the_flag()</code> — nothing else.</p>\n",
    "        \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(\n",
    "            label=\"Your Prompt\",\n",
    "            placeholder=\"Type anything. It will be sent directly to capture_the_flag().\",\n",
    "            autofocus=True,\n",
    "            lines=2,\n",
    "            elem_id=\"prompt_box\",\n",
    "        )\n",
    "\n",
    "    result = gr.Textbox(\n",
    "        label=\"Result\",\n",
    "        lines=16,\n",
    "        interactive=False,\n",
    "        elem_id=\"result_box\",\n",
    "        show_copy_button=True,\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        send = gr.Button(\"➤ Send\", variant=\"primary\", scale=2)\n",
    "        clear = gr.Button(\"🧹 Clear\", variant=\"secondary\")\n",
    "\n",
    "    # Wire events (unchanged behavior)\n",
    "    prompt.submit(run_ctf, inputs=prompt, outputs=result)\n",
    "    send.click(run_ctf, inputs=prompt, outputs=result)\n",
    "    clear.click(lambda: (\"\", \"\"), inputs=None, outputs=[prompt, result], queue=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba01fe6",
   "metadata": {},
   "source": [
    "# Agent with local Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095bde3",
   "metadata": {},
   "source": [
    "Note that for this part I downloaded the Mistral model on my laptop and ran the code locally through a 4090 gpu on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec5b4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NEW: local Mistral client shim that mimics OpenAI's chat.completions.create\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class _SimpleMessage:\n",
    "    def __init__(self, content: str):\n",
    "        self.content = content\n",
    "\n",
    "class _SimpleChoice:\n",
    "    def __init__(self, content: str):\n",
    "        self.message = _SimpleMessage(content)\n",
    "\n",
    "class _SimpleResponse:\n",
    "    def __init__(self, content: str):\n",
    "        self.choices = [_SimpleChoice(content)]\n",
    "\n",
    "class _Completions:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def create(self, model, messages, max_tokens=300, temperature=0.7):\n",
    "        # Build a chat-formatted prompt using the tokenizer's chat template\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        generated = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "        text = self.tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "        return _SimpleResponse(text)\n",
    "\n",
    "class _Chat:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.completions = _Completions(model, tokenizer)\n",
    "\n",
    "class LocalMistralClient:\n",
    "    def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        # Try to use 4-bit quantization if available; otherwise fall back to fp16.\n",
    "        try:\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config=quant_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        except Exception:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "\n",
    "        # Expose an OpenAI-like surface\n",
    "        self.chat = _Chat(self.model, self.tokenizer)\n",
    "\n",
    "\n",
    "# ===== YOUR ORIGINAL CLASS, MINIMALLY MODIFIED IN __init__ ONLY =====\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, api_token=\"tpsg-7SfPTJFEkQZNDYc7NjLOZ3c34UDuCqM\"):\n",
    "        self.api_token = api_token\n",
    "        self.model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "        # CHANGED: use local Mistral client instead of OpenAI API client\n",
    "        self.client = LocalMistralClient(self.model)\n",
    "        self.conversation = []  # List of (role, content)\n",
    "        self.summary = None\n",
    "        self.max_turns_before_summarizing = 2  # Number of user-assistant pairs\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def capture_the_flag(self, prompt: str) -> str:\n",
    "        # Add current user prompt to conversation\n",
    "        reply = \"\"\n",
    "        self.conversation.append((\"user\", prompt))\n",
    "        \n",
    "        \n",
    "        # Check if summarization is needed\n",
    "        \n",
    "        if self.needs_summarization():\n",
    "            self.update_memory()\n",
    "\n",
    "        \n",
    "\n",
    "        # Build prompt with context\n",
    "        if self.check_if_pdf_input(prompt):\n",
    "            \n",
    "             \n",
    "            path=self.extract_local_pdf_paths(prompt)\n",
    "            chunks,embeddings=self.pdf_address_to_chunk_embedding(address=path)\n",
    "            index, chunks=self.build_or_load_faiss_index(chunks=chunks,embeddings=embeddings)\n",
    "\n",
    "\n",
    "            query = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"I am going to give you a prompt. After reading the prompt write a single query based on the given prompt so that I can look for the answer of that query in the pdf file. prompt:-{prompt}-. Just print the query.\"}],\n",
    "                max_tokens=300,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "            query = query.choices[0].message.content.strip()\n",
    "\n",
    "            \n",
    "\n",
    "            results = self.search_faiss_index(index, chunks, query)\n",
    "            \n",
    "\n",
    "            reply = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"This is the query that I have: {query}.These are the relevant answers I found based on the query: {results}. Now based on the provided information give a answer to the query.\"}],\n",
    "                max_tokens=300,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            reply = reply.choices[0].message.content.strip()\n",
    "\n",
    "        elif self.game_mode(prompt=prompt):\n",
    "            try:\n",
    "                play()\n",
    "            except SystemExit as e:\n",
    "                # Avoid printing traceback in Jupyter when exiting with a message\n",
    "                if isinstance(e.code, str):\n",
    "                    print(e.code)\n",
    "                raise  # re-raise so IPython doesn't crash with weird traceback\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nGame aborted. Bye!\")\n",
    "            \n",
    "\n",
    "        elif self.check_for_web_search(prompt=prompt):\n",
    "\n",
    "            reply=self.web_searcher(prompt)\n",
    "\n",
    "            self.conversation.append((\"assistant\", reply))\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            messages = self.build_prompt(prompt)\n",
    "\n",
    "            # Call model\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                max_tokens=300,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "            reply = response.choices[0].message.content.strip()\n",
    "            self.conversation.append((\"assistant\", reply))\n",
    "\n",
    "        return reply\n",
    "\n",
    "    def needs_summarization(self) -> bool:\n",
    "        # If too many total messages (user+assistant), trigger summarization\n",
    "        return len(self.conversation) > self.max_turns_before_summarizing * 2\n",
    "\n",
    "    def update_memory(self):\n",
    "        # Summarize everything except the most recent few turns\n",
    "        old_convo = self.conversation[:-self.max_turns_before_summarizing]\n",
    "        if not old_convo:\n",
    "            return\n",
    "\n",
    "        convo_text = \"\\n\".join(f\"{role.capitalize()}: {content}\" for role, content in old_convo)\n",
    "\n",
    "        summary_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a summarizer.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following conversation for memory:\\n\\n{convo_text}\"}\n",
    "        ]\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=summary_prompt,\n",
    "            max_tokens=200,\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "        self.summary = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Retain only the recent portion of the conversation\n",
    "        self.conversation = self.conversation[-self.max_turns_before_summarizing:]\n",
    "\n",
    "    def build_prompt(self, current_user_input: str):\n",
    "\n",
    "        messages = []\n",
    "        \n",
    "        if self.summary:\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"Summary of prior conversation: {self.summary}\"\n",
    "            })\n",
    "\n",
    "        # Add recent conversation history\n",
    "        for role, content in self.conversation:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "        # Add current prompt again as the latest user message\n",
    "        # (optional if already added in self.conversation)\n",
    "        # messages.append({\"role\": \"user\", \"content\": current_user_input})\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def check_for_web_search(self,prompt):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"{prompt}. if you need more data to answer user question only say SEARCH.\"}] ,\n",
    "            max_tokens=100,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        response = response.choices[0].message.content.strip()\n",
    "\n",
    "        #print(response)\n",
    "        if response == 'SEARCH':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def web_searcher(self,prompt):\n",
    "\n",
    "        num_results=3\n",
    "        #message=self.build_prompt(prompt)\n",
    "        query = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"in order to answer the following prompt write the best query so i can search the internet with that query {prompt}. just write the query\"}],\n",
    "            max_tokens=100,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        query=query.choices[0].message.content.strip()\n",
    "        \n",
    "        EXA_API_KEY = \"679619bb-2cb6-4e88-8ee1-33febc5ffbac\"\n",
    "        EXA_ENDPOINT = \"https://api.exa.ai/search\"\n",
    "\n",
    "        headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key\": EXA_API_KEY\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "        payload = {\n",
    "            \"query\": query,\n",
    "            \"num_results\": num_results,\n",
    "            \"include_domains\": [],\n",
    "            \"exclude_domains\": [],\n",
    "            \"use_autoprompt\": True,   # optional for better queries\n",
    "            \"type\": \"auto\"            \n",
    "        }\n",
    "\n",
    "        response = requests.post(EXA_ENDPOINT, json=payload, headers=headers,proxies={})\n",
    "        response.raise_for_status()\n",
    "\n",
    "        results = response.json()[\"results\"]\n",
    "        urls = [item[\"url\"] for item in results]\n",
    "        contents=[]\n",
    "        headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "        }   \n",
    "        \n",
    "        for url in urls:\n",
    "            response = requests.get(url,headers=headers)\n",
    "            response.raise_for_status()  # raises HTTPError if status is 4xx or 5xx\n",
    "            content = response.text\n",
    "            contents.append(content)\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"{query}.for the query that i just gave you i have searched the internet and i am going to provide you with the content of the three most relevant results.find the best answer for the query in the contents of these web sites  {contents}. just write the answer\"}],\n",
    "            max_tokens=100,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        response=response.choices[0].message.content.strip()\n",
    "\n",
    "        return response\n",
    "    \n",
    "    \n",
    "    def pdf_address_to_chunk_embedding(self,address,chunk_len=250):\n",
    "        \n",
    "        \n",
    "       \n",
    "        nltk.download('punkt_tab')\n",
    "  \n",
    "\n",
    "        doc = fitz.open(address)\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "        doc.close()\n",
    "\n",
    "        # Step 2: Tokenize and chunk into 100-word chunks\n",
    "        words = word_tokenize(full_text)\n",
    "        chunks = [' '.join(words[i:i + chunk_len])\n",
    "                  for i in range(0, len(words), chunk_len)]\n",
    "        # compute embeddings as numpy float32 array\n",
    "        embeddings = self.embedding_model.encode(chunks)\n",
    "        embeddings = np.asarray(embeddings, dtype='float32')\n",
    "\n",
    "        return chunks,embeddings\n",
    "    \n",
    "\n",
    "\n",
    "    def build_or_load_faiss_index(self,chunks, embeddings, index_path=\"faiss.index\"):\n",
    "       \n",
    "        dim = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "        index.add(embeddings)                 # now a proper numpy array\n",
    "        faiss.write_index(index, index_path)\n",
    "        return index, chunks\n",
    "\n",
    "    def search_faiss_index(self,index, chunks, query, top_k=3):\n",
    "\n",
    "        q_emb = self.embedding_model.encode([query])\n",
    "        q_emb = np.asarray(q_emb, dtype='float32')\n",
    "        distances, indices = index.search(q_emb, top_k)\n",
    "        print(indices)\n",
    "        return [chunks[i] for i in indices[0]]\n",
    "\n",
    "    def check_if_pdf_input(self,query):\n",
    "        return '.pdf' in query.lower()\n",
    "    \n",
    "    def extract_local_pdf_paths(self,text):\n",
    "        # Match file paths with spaces, until .pdf\n",
    "        pattern = r'([A-Za-z]:[\\\\/](?:[^:*?\"<>|\\r\\n]+[\\\\/])*[^:*?\"<>|\\r\\n]+\\.pdf)'\n",
    "        matches = re.findall(pattern, text)\n",
    "        pdf_paths = [match.strip() for match in matches if os.path.exists(match.strip())]\n",
    "        return pdf_paths[0] if pdf_paths else None\n",
    "    def game_mode(self,prompt):\n",
    "\n",
    "        # response = self.client.chat.completions.create(\n",
    "        #     model=self.model,\n",
    "        #     messages=[{\"role\": \"user\", \"content\": f\"Read this user input: -{prompt}-.If user input contain words like: game/20 question/20 question game/guess, just print : GAME\"}],\n",
    "        #     max_tokens=100,\n",
    "        #     temperature=0.1\n",
    "        # )\n",
    "        \n",
    "        # response=response.choices[0].message.content.strip()\n",
    "\n",
    "        # if response==\"GAME\":\n",
    "        #     return True\n",
    "        \n",
    "        # return False\n",
    "        return '20 question game' in prompt.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c647d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b2fc2dc7aa4f07a0491b28e815ebb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c1af0cd1694367a2cf25623b264731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PT\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "agent=Agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb4ad0",
   "metadata": {},
   "source": [
    "# Sample outputs to show the Agent's abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a514eb",
   "metadata": {},
   "source": [
    "1.ordinari conversation\n",
    "2.reading the paper\n",
    "3.searching the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7fb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model name is Mistral.\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('what is your model name?')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279fc3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 7 parameters that control my behavior: learning rate, batch size, number of epochs, regularization strength, optimizer, activation function, and loss function.\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('How many parameters do you have?')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba4aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"Large Language Diffusion Models\" was authored by Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, and Ji-Rong Wen.\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('Who wrote the paper d://uni stuff//Semester 6//Deep Learning//Project//Large Language Diffusion Models.pdf')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c1a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kylian Mbappé got off to a perfect start in his second season with Real Madrid, leading the team to a 1-0 victory over Osasuna in their Spanish league opener Tuesday.\n"
     ]
    }
   ],
   "source": [
    "ans=agent.capture_the_flag('Who scored in the last Real Madrid game?')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba94659",
   "metadata": {},
   "source": [
    "As you can see by changing the model to a local one the Agent work just as good as before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
